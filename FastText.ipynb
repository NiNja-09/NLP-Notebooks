{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FastText\n",
        "\n",
        "FastText is an extension of Word2Vec, developed by Facebook AI Research, that introduces subword information into the word embeddings. It breaks down words into smaller subword units called \"character n-grams,\" which enables it to capture the meaning of words even if they are out of vocabulary or rare.\n",
        "\n",
        "### How FastText Works:\n",
        "\n",
        "Subword Representation:\n",
        "FastText represents each word as a bag of character n-grams (including the word itself). For example, for the word \"apple,\" with a character n-gram range of 3, it would include \"app,\" \"ppl,\" \"ple,\" and \"apple\".\n",
        "\n",
        "Embedding Generation:\n",
        "FastText generates embeddings for each subword using a shallow neural network. It learns vectors for each subword, which are then averaged to obtain the final word vector representation.\n",
        "\n",
        "Training:\n",
        "FastText trains these subword embeddings using a variant of the Skipgram model. It tries to predict the context words given the subwords in the input.\n",
        "\n",
        "Hierarchical Softmax:\n",
        "FastText employs hierarchical softmax for efficiency in training. Instead of calculating the softmax probability for all words in the vocabulary, it constructs a binary tree based on word frequency, and the probability is computed hierarchically.\n",
        "\n",
        "### Example -\n",
        "\n",
        "corpus - \"the cat sat on the mat\"\n",
        "\n",
        "After tokenization and preprocessing, the corpus becomes:\n",
        "\n",
        "[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "\n",
        "Now, let's generate character n-grams (trigrams in this example):\n",
        "\n",
        "[\"<th\", \"the\", \"he>\", \"<ca\", \"cat\", \"at>\", \"<sa\", \"sat\", \"at>\", \"<on\", \"on>\", \"<th\", \"the\", \"he>\", \"<ma\", \"mat\", \"at>\"]\n",
        "\n",
        "Next, we create word representations by summing up the vectors of the constituent character n-grams. For simplicity, let's assume each character n-gram is represented by a randomly initialized vector.\n",
        "\n",
        "For example, the word \"the\" may be represented as:\n",
        "\n",
        "\"the\" = \"<th\" + \"the\" + \"he>\" = [0.2, 0.4, -0.1] + [0.5, 0.3, 0.2] + [0.1, -0.2, 0.4] = [0.8, 0.5, 0.5]\n",
        "\n",
        "Similarly, we compute representations for other words in the corpus.\n",
        "\n",
        "During training, FastText adjusts these word representations to predict context words given target words, or vice versa, effectively capturing semantic and morphological similarities between words.\n",
        "\n",
        "Finally, after training, FastText provides dense vector representations (embeddings) for each word, which can be used for various NLP tasks.\n",
        "\n",
        "### Advantages:\n",
        "Handling Out-of-Vocabulary Words: FastText can generate embeddings for out-of-vocabulary words by leveraging their subword information.\n",
        "\n",
        "Better Performance for Morphologically Rich Languages: It performs well for languages with complex morphology, where words may share common roots or affixes.\n",
        "\n",
        "Efficient Training: FastText's hierarchical softmax and subword representations lead to faster training compared to traditional word embeddings.\n",
        "\n",
        "### Disadvantages:\n",
        "Increased Memory Usage: Including subword information increases the memory footprint of the model, especially for large vocabularies.\n",
        "\n",
        "Complexity in Interpretation: The subword embeddings add complexity to the interpretation of the learned embeddings, making it harder to understand the semantic relationships between words.\n",
        "\n",
        "### Applications:\n",
        "Text Classification: FastText is commonly used in text classification tasks, where it can capture the semantic and syntactic information present in text documents efficiently.\n",
        "\n",
        "Information Retrieval: It's effective in search engines and recommendation systems for understanding the meaning of queries or documents.\n",
        "\n",
        "Language Translation: FastText embeddings can improve the performance of machine translation systems by capturing the morphological variations of words in different languages.\n",
        "\n",
        "### Difference from Others:\n",
        "Subword Information: FastText incorporates subword information, making it more robust to handle morphologically rich languages and out-of-vocabulary words compared to traditional word embedding methods like Word2Vec and GloVe."
      ],
      "metadata": {
        "id": "In-b4MAD6vpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using gensim Library (Building from Scratch)"
      ],
      "metadata": {
        "id": "pLjpN9jdQAJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yluwoIUK6spx"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "from gensim.utils import simple_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"FastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers.\",\n",
        "    \"It works on standard, generic hardware and has been widely adopted by the research community and industry.\",\n",
        "    \"FastText is capable of learning vector representations for words, phrases, and entire sentences.\",\n",
        "    \"It can be used to perform various NLP tasks such as text classification, sentiment analysis, and word similarity.\",\n",
        "    \"The models trained with FastText are fast and memory-efficient, making them suitable for large-scale text data.\",\n",
        "    \"In addition to its speed and efficiency, FastText also supports subword embeddings, which can capture morphological information.\",\n",
        "    \"Overall, FastText is a powerful tool for text representation learning and NLP tasks, and it continues to be actively developed and improved.\",\n",
        "    \"To get started with FastText, you can install it using pip and explore its functionalities through tutorials and documentation.\",\n",
        "    \"Experimenting with different parameters and training on diverse datasets can help you achieve better performance with FastText.\"\n",
        "]"
      ],
      "metadata": {
        "id": "OuEpEP3jNMsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the corpus\n",
        "tokenized_corpus = [simple_preprocess(doc) for doc in corpus]"
      ],
      "metadata": {
        "id": "RqAaxtffNRQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train FastText model\n",
        "model = FastText(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, sg=1)  # Use skip-gram (sg=1)"
      ],
      "metadata": {
        "id": "l3xTnTJwNTb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access word vectors\n",
        "print(\"Word vector for 'library':\", model.wv['library'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18G8CKgRNXeR",
        "outputId": "3e53fbb4-f190-40ec-e108-757d697534cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'library': [-8.8077453e-05  5.3281663e-04 -1.2101445e-03 -8.4103507e-05\n",
            " -1.2615649e-04  1.1037531e-03  1.1210773e-03 -1.1594556e-03\n",
            "  9.6220477e-04  4.5948726e-04  2.2894773e-03 -2.9825934e-04\n",
            " -2.3649624e-04 -5.3622312e-04 -3.5925646e-04  7.6839986e-04\n",
            " -7.1130582e-04 -2.0754895e-04  1.3591407e-03  1.2039577e-04\n",
            " -4.1452641e-04 -1.0393960e-03 -1.3183668e-04 -8.1285770e-04\n",
            " -3.4140667e-06  7.0950011e-04  1.2916297e-03  9.3733368e-04\n",
            " -4.6578297e-04  8.3560200e-04  9.0844190e-04  1.7968602e-03\n",
            " -1.2718287e-04 -6.3243060e-05 -9.2741894e-04  1.9835065e-04\n",
            " -2.0102574e-03  1.4181191e-03  1.2622278e-03 -2.9321088e-04\n",
            "  1.3147315e-03 -4.2026918e-04 -5.1464216e-04 -2.4142396e-03\n",
            " -1.6289677e-03  1.3099272e-04 -1.3722297e-03 -6.6590519e-04\n",
            "  5.5360986e-04  5.9346895e-04 -6.2162057e-04 -2.6961989e-03\n",
            "  5.5239687e-04 -2.2099153e-03 -3.9787550e-04  2.3126917e-05\n",
            " -2.0825106e-04  1.3279810e-03 -3.2227003e-04  9.0600504e-04\n",
            " -2.7588487e-05  2.2124976e-03 -2.2200315e-04  9.1538706e-04\n",
            "  4.0596101e-04  1.6189723e-03 -5.5752217e-04 -6.8272202e-04\n",
            "  2.7699376e-04 -3.6445301e-04 -6.6341268e-04  2.4157544e-04\n",
            "  1.9782716e-04  1.6276308e-03 -1.4383874e-03 -1.3503681e-03\n",
            "  6.9551919e-05 -7.7258273e-06 -6.1741640e-04  2.1055310e-03\n",
            "  1.0136798e-03 -1.7460099e-04  1.2430688e-03  9.7567966e-04\n",
            "  8.3318574e-04 -2.2109328e-03 -2.5713249e-04  8.2929723e-04\n",
            " -2.3610711e-03  4.2847430e-04 -8.6726161e-04 -4.4448179e-04\n",
            " -4.6123780e-04 -1.9073002e-03 -3.4377951e-08  9.3007897e-04\n",
            "  1.6150769e-04 -5.4719619e-04  1.2008757e-03 -6.7258277e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get most similar words\n",
        "print(\"Most similar words to 'library':\", model.wv.most_similar(\"library\", topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYwgpLFMNZ9A",
        "outputId": "be12dcf1-1ca6-4c11-d61f-e7ce0e1aa1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'library': [('learn', 0.2163046896457672), ('the', 0.17864620685577393), ('efficiency', 0.16718940436840057), ('efficient', 0.15956450998783112), ('subword', 0.15050262212753296)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using FastText library"
      ],
      "metadata": {
        "id": "xo9J_PUDQJCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext"
      ],
      "metadata": {
        "id": "tSzTismVNbsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"FastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers.\",\n",
        "    \"It works on standard, generic hardware and has been widely adopted by the research community and industry.\",\n",
        "    \"FastText is capable of learning vector representations for words, phrases, and entire sentences.\",\n",
        "    \"It can be used to perform various NLP tasks such as text classification, sentiment analysis, and word similarity.\",\n",
        "    \"The models trained with FastText are fast and memory-efficient, making them suitable for large-scale text data.\",\n",
        "    \"In addition to its speed and efficiency, FastText also supports subword embeddings, which can capture morphological information.\",\n",
        "    \"Overall, FastText is a powerful tool for text representation learning and NLP tasks, and it continues to be actively developed and improved.\",\n",
        "    \"To get started with FastText, you can install it using pip and explore its functionalities through tutorials and documentation.\",\n",
        "    \"Experimenting with different parameters and training on diverse datasets can help you achieve better performance with FastText.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "VkGNhIw7NwXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the corpus to a text file\n",
        "with open('corpus.txt', 'w') as f:\n",
        "    for sentence in corpus:\n",
        "        f.write(sentence + '\\n')"
      ],
      "metadata": {
        "id": "JpdyAezxOPsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat corpus.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK1GKbidPQ75",
        "outputId": "2e1e3bd1-ba2c-44d2-90aa-a61a886ea36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers.\n",
            "It works on standard, generic hardware and has been widely adopted by the research community and industry.\n",
            "FastText is capable of learning vector representations for words, phrases, and entire sentences.\n",
            "It can be used to perform various NLP tasks such as text classification, sentiment analysis, and word similarity.\n",
            "The models trained with FastText are fast and memory-efficient, making them suitable for large-scale text data.\n",
            "In addition to its speed and efficiency, FastText also supports subword embeddings, which can capture morphological information.\n",
            "Overall, FastText is a powerful tool for text representation learning and NLP tasks, and it continues to be actively developed and improved.\n",
            "To get started with FastText, you can install it using pip and explore its functionalities through tutorials and documentation.\n",
            "Experimenting with different parameters and training on diverse datasets can help you achieve better performance with FastText.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train FastText model\n",
        "model = fasttext.train_unsupervised('corpus.txt', model='skipgram')"
      ],
      "metadata": {
        "id": "AaANY-j0OR3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access word vectors\n",
        "print(\"Word vector for 'library':\", model.get_word_vector('library'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N56ss9M9OTsJ",
        "outputId": "e83beb0d-b6a2-409f-fae9-241dc6db58be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'library': [ 2.7375986e-04  5.9964390e-05 -1.2389838e-04  2.9738454e-04\n",
            " -4.0860128e-04 -3.2297074e-04 -2.2612902e-04 -4.9441954e-04\n",
            "  2.1073967e-04  3.6678519e-04  3.0976799e-04 -6.8614166e-04\n",
            " -2.4202529e-04  4.0067913e-04 -2.1648475e-04  2.2100871e-04\n",
            "  3.5922384e-04 -7.2761375e-04 -3.5948397e-04  2.7200754e-04\n",
            "  1.5242146e-04 -4.4802483e-04  3.1772148e-04  2.9537667e-04\n",
            " -1.2225512e-04 -1.6157531e-04 -5.9167051e-04 -9.0905552e-05\n",
            "  1.5479853e-04 -4.2852381e-04  5.4930814e-04  6.9145177e-04\n",
            "  2.6214027e-04 -3.7651384e-04  6.2804669e-04  1.7067177e-04\n",
            "  2.5638807e-04 -3.8014483e-04 -2.1241744e-04  3.4406758e-04\n",
            " -8.6058084e-05 -3.2095602e-04 -1.2726686e-04 -1.0489493e-04\n",
            "  2.0620592e-04 -1.6819664e-04 -2.7886571e-04 -1.6952082e-04\n",
            " -1.1267147e-04 -3.0912965e-05 -1.1646561e-04  1.8674570e-04\n",
            " -6.6801251e-05 -3.1414209e-04 -1.2539179e-04 -1.5214071e-04\n",
            "  5.3448818e-04  7.1946037e-04 -1.5170920e-05 -6.2955986e-04\n",
            " -7.4522912e-05  4.0849671e-04 -3.1218646e-04  3.2858305e-05\n",
            " -3.9261687e-04 -7.1033515e-04  1.6358343e-05  1.8378966e-04\n",
            " -5.7772390e-04  2.0226142e-04  4.4019890e-04 -6.4253851e-05\n",
            " -1.6503401e-04 -5.5789022e-04 -3.4345413e-04 -1.4375748e-05\n",
            " -2.6456200e-04 -1.8859570e-04  2.3683311e-05 -2.9753335e-04\n",
            " -4.3661529e-04  1.7979140e-04 -6.0829992e-05  4.7791578e-04\n",
            " -7.5962767e-04  1.3671008e-04  2.8679316e-04 -2.7054298e-04\n",
            " -1.1309114e-04 -4.5766920e-04  2.5820083e-04  5.0857011e-04\n",
            "  3.3694363e-04  2.5476963e-04 -4.5914899e-04 -3.2368067e-04\n",
            " -2.3731582e-06 -1.1608350e-04 -1.7247029e-04  3.4587818e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get most similar words\n",
        "print(\"Most similar words to 'library':\", model.get_nearest_neighbors('library', k=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVnObZbcOhwZ",
        "outputId": "68bf6f0f-ef76-4858-8165-38156eab6fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'library': [(0.13605175912380219, 'text'), (0.11700049042701721, 'FastText')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXB7Nf2yOkMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}