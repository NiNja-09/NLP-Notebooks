{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization is the process of breaking down text into smaller units, typically words or subwords, called tokens.\n",
        "These tokens serve as the basic building blocks for further analysis in natural language processing (NLP) tasks.\n",
        "Different tokenization methods are available in NLTK (Natural Language Toolkit) to suit various needs and scenarios.\n",
        "\n",
        "Structure of Token -> prefix - morpheme - suffix\n",
        "\n",
        "Corpus > Document > Paragraph > Sentence > Tokens\n",
        "\n",
        "#### How Tokenization Works:\n",
        "\n",
        "Sentence Tokenization: This step involves splitting the text corpus into individual sentences. This can typically be done using punctuation marks like periods, exclamation marks, and question marks as delimiters.\n",
        "\n",
        "Example: \"Hello! How are you today?\" -> [\"Hello!\", \"How are you today?\"]\n",
        "\n",
        "Word Tokenization: Once sentences are separated, the next step is to break down each sentence into individual words or tokens. This is usually done by splitting the sentences based on whitespace or punctuation.\n",
        "\n",
        "Example: \"How are you today?\" -> [\"How\", \"are\", \"you\", \"today?\"]\n",
        "\n",
        "Further Tokenization: Depending on the task and requirements, tokenization can be extended to include additional levels of granularity such as splitting hyphenated words, handling contractions, or identifying special characters.\n",
        "\n",
        "\n",
        "    Examples of different types of tokenization method -\n",
        "    sent_tokenize,\n",
        "    word_tokenize,\n",
        "    TreebankWordTokenizer,\n",
        "    wordpunct_tokenize,\n",
        "    WhitespaceTokenizer,\n",
        "    TweetTokenizer,\n",
        "    casual_tokenize,\n",
        "    PunktSentenceTokenizer,\n",
        "    ReppTokenizer\n",
        "\n",
        "\n",
        "#### Advantages\n",
        "\n",
        "Text Preprocessing: Tokenization is an essential step in text preprocessing for various NLP tasks, including sentiment analysis, machine translation, and information retrieval.\n",
        "\n",
        "Normalization: It helps in standardizing the text data by breaking it down into standardized units, facilitating further analysis and processing.\n",
        "\n",
        "Feature Extraction: Tokens serve as the basis for feature extraction in NLP models, enabling the extraction of meaningful information from text data.\n",
        "\n",
        "#### Disadvantages\n",
        "\n",
        "Ambiguity: Tokenization may encounter ambiguity in certain cases, such as tokenizing compound words or handling abbreviations, leading to potential errors in downstream tasks.\n",
        "\n",
        "Language-specific Challenges: Tokenization may face challenges in languages with complex word structures, morphological variations, or non-standard orthographies.\n",
        "\n",
        "Tokenization Errors: Errors in tokenization, such as splitting words incorrectly or treating punctuation inconsistently, can impact the quality of subsequent analyses and results.\n"
      ],
      "metadata": {
        "id": "aFGSdHafY6wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, TreebankWordTokenizer, wordpunct_tokenize, WhitespaceTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ-w26moOZoj",
        "outputId": "f4d6fd4c-4d7c-4541-8029-23fd19debe8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "In the domain of natural language processing (NLP), statistical NLP in particular, there's a need to train the model or algorithm with lots of data.\n",
        "For this purpose, researchers have assembled many text corpora.\n",
        "A common corpus is also useful for benchmarking models.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mr4P-47eRB8k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_VFCnkJRhD8",
        "outputId": "450e918a-2e28-4315-84c6-60dfd075aa66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "In the domain of natural language processing (NLP), statistical NLP in particular, there's a need to train the model or algorithm with lots of data.\n",
            "For this purpose, researchers have assembled many text corpora.\n",
            "A common corpus is also useful for benchmarking models.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sent_tokenize"
      ],
      "metadata": {
        "id": "oWZ-tik3b1-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = sent_tokenize(corpus)\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9OSHL2ERivj",
        "outputId": "adf051cd-bbc5-42c4-8f72-9d6d12490550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"\\nIn the domain of natural language processing (NLP), statistical NLP in particular, there's a need to train the model or algorithm with lots of data.\",\n",
              " 'For this purpose, researchers have assembled many text corpora.',\n",
              " 'A common corpus is also useful for benchmarking models.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_tokenize"
      ],
      "metadata": {
        "id": "l9gGaF-_b8Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(corpus)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmMaU0OXR4fj",
        "outputId": "1926018a-40e0-4326-812a-ca8c78e37f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'the',\n",
              " 'domain',\n",
              " 'of',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " ',',\n",
              " 'statistical',\n",
              " 'NLP',\n",
              " 'in',\n",
              " 'particular',\n",
              " ',',\n",
              " 'there',\n",
              " \"'s\",\n",
              " 'a',\n",
              " 'need',\n",
              " 'to',\n",
              " 'train',\n",
              " 'the',\n",
              " 'model',\n",
              " 'or',\n",
              " 'algorithm',\n",
              " 'with',\n",
              " 'lots',\n",
              " 'of',\n",
              " 'data',\n",
              " '.',\n",
              " 'For',\n",
              " 'this',\n",
              " 'purpose',\n",
              " ',',\n",
              " 'researchers',\n",
              " 'have',\n",
              " 'assembled',\n",
              " 'many',\n",
              " 'text',\n",
              " 'corpora',\n",
              " '.',\n",
              " 'A',\n",
              " 'common',\n",
              " 'corpus',\n",
              " 'is',\n",
              " 'also',\n",
              " 'useful',\n",
              " 'for',\n",
              " 'benchmarking',\n",
              " 'models',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words in sentence:\n",
        "  words = word_tokenize(words)\n",
        "  print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy7B7Wk2SMcb",
        "outputId": "86421cd8-3044-4dbc-9314-43d555954053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'domain', 'of', 'natural', 'language', 'processing', '(', 'NLP', ')', ',', 'statistical', 'NLP', 'in', 'particular', ',', 'there', \"'s\", 'a', 'need', 'to', 'train', 'the', 'model', 'or', 'algorithm', 'with', 'lots', 'of', 'data', '.']\n",
            "['For', 'this', 'purpose', ',', 'researchers', 'have', 'assembled', 'many', 'text', 'corpora', '.']\n",
            "['A', 'common', 'corpus', 'is', 'also', 'useful', 'for', 'benchmarking', 'models', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wordpunct_tokenize"
      ],
      "metadata": {
        "id": "HC0lgVQdcCON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WordPunc = wordpunct_tokenize(corpus)\n",
        "WordPunc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YxFD3zKSR-b",
        "outputId": "37442a9e-0946-40fc-f712-9f8579d56835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'the',\n",
              " 'domain',\n",
              " 'of',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " '),',\n",
              " 'statistical',\n",
              " 'NLP',\n",
              " 'in',\n",
              " 'particular',\n",
              " ',',\n",
              " 'there',\n",
              " \"'\",\n",
              " 's',\n",
              " 'a',\n",
              " 'need',\n",
              " 'to',\n",
              " 'train',\n",
              " 'the',\n",
              " 'model',\n",
              " 'or',\n",
              " 'algorithm',\n",
              " 'with',\n",
              " 'lots',\n",
              " 'of',\n",
              " 'data',\n",
              " '.',\n",
              " 'For',\n",
              " 'this',\n",
              " 'purpose',\n",
              " ',',\n",
              " 'researchers',\n",
              " 'have',\n",
              " 'assembled',\n",
              " 'many',\n",
              " 'text',\n",
              " 'corpora',\n",
              " '.',\n",
              " 'A',\n",
              " 'common',\n",
              " 'corpus',\n",
              " 'is',\n",
              " 'also',\n",
              " 'useful',\n",
              " 'for',\n",
              " 'benchmarking',\n",
              " 'models',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words in sentence:\n",
        "  words = wordpunct_tokenize(words)\n",
        "  print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKtP2ZxTSvBr",
        "outputId": "ce3b4ed0-bc56-4b36-c887-667b763960dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'domain', 'of', 'natural', 'language', 'processing', '(', 'NLP', '),', 'statistical', 'NLP', 'in', 'particular', ',', 'there', \"'\", 's', 'a', 'need', 'to', 'train', 'the', 'model', 'or', 'algorithm', 'with', 'lots', 'of', 'data', '.']\n",
            "['For', 'this', 'purpose', ',', 'researchers', 'have', 'assembled', 'many', 'text', 'corpora', '.']\n",
            "['A', 'common', 'corpus', 'is', 'also', 'useful', 'for', 'benchmarking', 'models', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "pSV1XM4EcExv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G9xs0-LTcNi",
        "outputId": "7931e2c6-87ec-41b9-9d8d-3e0cfaf9b15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'the',\n",
              " 'domain',\n",
              " 'of',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " ',',\n",
              " 'statistical',\n",
              " 'NLP',\n",
              " 'in',\n",
              " 'particular',\n",
              " ',',\n",
              " 'there',\n",
              " \"'s\",\n",
              " 'a',\n",
              " 'need',\n",
              " 'to',\n",
              " 'train',\n",
              " 'the',\n",
              " 'model',\n",
              " 'or',\n",
              " 'algorithm',\n",
              " 'with',\n",
              " 'lots',\n",
              " 'of',\n",
              " 'data.',\n",
              " 'For',\n",
              " 'this',\n",
              " 'purpose',\n",
              " ',',\n",
              " 'researchers',\n",
              " 'have',\n",
              " 'assembled',\n",
              " 'many',\n",
              " 'text',\n",
              " 'corpora.',\n",
              " 'A',\n",
              " 'common',\n",
              " 'corpus',\n",
              " 'is',\n",
              " 'also',\n",
              " 'useful',\n",
              " 'for',\n",
              " 'benchmarking',\n",
              " 'models',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WhitespaceTokenizer"
      ],
      "metadata": {
        "id": "p13KqEEvcINQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(WhitespaceTokenizer().span_tokenize(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pnjnml2TzcL",
        "outputId": "732154a9-22ec-4869-ea51-3e7382e122e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 3),\n",
              " (4, 7),\n",
              " (8, 14),\n",
              " (15, 17),\n",
              " (18, 25),\n",
              " (26, 34),\n",
              " (35, 45),\n",
              " (46, 52),\n",
              " (53, 64),\n",
              " (65, 68),\n",
              " (69, 71),\n",
              " (72, 83),\n",
              " (84, 91),\n",
              " (92, 93),\n",
              " (94, 98),\n",
              " (99, 101),\n",
              " (102, 107),\n",
              " (108, 111),\n",
              " (112, 117),\n",
              " (118, 120),\n",
              " (121, 130),\n",
              " (131, 135),\n",
              " (136, 140),\n",
              " (141, 143),\n",
              " (144, 149),\n",
              " (150, 153),\n",
              " (154, 158),\n",
              " (159, 167),\n",
              " (168, 179),\n",
              " (180, 184),\n",
              " (185, 194),\n",
              " (195, 199),\n",
              " (200, 204),\n",
              " (205, 213),\n",
              " (214, 215),\n",
              " (216, 222),\n",
              " (223, 229),\n",
              " (230, 232),\n",
              " (233, 237),\n",
              " (238, 244),\n",
              " (245, 248),\n",
              " (249, 261),\n",
              " (262, 269)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "whitespace = WhitespaceTokenizer()\n",
        "whitespace.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3Ik1hVYUd1r",
        "outputId": "3ad8b247-9276-4fe2-bea1-03685d72a969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'the',\n",
              " 'domain',\n",
              " 'of',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(NLP),',\n",
              " 'statistical',\n",
              " 'NLP',\n",
              " 'in',\n",
              " 'particular,',\n",
              " \"there's\",\n",
              " 'a',\n",
              " 'need',\n",
              " 'to',\n",
              " 'train',\n",
              " 'the',\n",
              " 'model',\n",
              " 'or',\n",
              " 'algorithm',\n",
              " 'with',\n",
              " 'lots',\n",
              " 'of',\n",
              " 'data.',\n",
              " 'For',\n",
              " 'this',\n",
              " 'purpose,',\n",
              " 'researchers',\n",
              " 'have',\n",
              " 'assembled',\n",
              " 'many',\n",
              " 'text',\n",
              " 'corpora.',\n",
              " 'A',\n",
              " 'common',\n",
              " 'corpus',\n",
              " 'is',\n",
              " 'also',\n",
              " 'useful',\n",
              " 'for',\n",
              " 'benchmarking',\n",
              " 'models.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using spaCy"
      ],
      "metadata": {
        "id": "GJXHNSs9cVct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "27Wer3QIVAmE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text using spaCy\n",
        "doc = nlp(corpus)"
      ],
      "metadata": {
        "id": "LzdY9rxxcaY-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Extract tokens\n",
        "tokens = [token.text for token in doc]"
      ],
      "metadata": {
        "id": "dRqP5RgOcf55"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrHfmb7WcmpR",
        "outputId": "34f4ad1c-27e5-40bb-ae74-454521baeea2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', 'In', 'the', 'domain', 'of', 'natural', 'language', 'processing', '(', 'NLP', ')', ',', 'statistical', 'NLP', 'in', 'particular', ',', 'there', \"'s\", 'a', 'need', 'to', 'train', 'the', 'model', 'or', 'algorithm', 'with', 'lots', 'of', 'data', '.', '\\n', 'For', 'this', 'purpose', ',', 'researchers', 'have', 'assembled', 'many', 'text', 'corpora', '.', '\\n', 'A', 'common', 'corpus', 'is', 'also', 'useful', 'for', 'benchmarking', 'models', '.', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8D4GyQxSdWCZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}