{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Avg Word2Vec\n",
        "\n",
        "Average Word2Vec is an approach to generate word embeddings by averaging the Word2Vec vectors of individual words in a text document. It captures the overall semantic meaning of the document by taking the average of the word vectors present in it.\n",
        "\n",
        "### How Avg Word2Vec Works:\n",
        "\n",
        "Word Vector Generation: First, Word2Vec is used to generate word vectors for each word in the corpus. Word2Vec typically generates high-dimensional vectors representing the semantic meaning of each word based on its context.\n",
        "\n",
        "Average Calculation: For a given text document, the word vectors of all the words present in the document are averaged to produce a single vector representation for the entire document. This average vector encapsulates the overall semantic information of the document.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Consider two text documents:\n",
        "\n",
        "\"I love dogs and cats.\"\n",
        "\n",
        "\"Dogs are loyal companions.\"\n",
        "\n",
        "For each document, we generate Word2Vec vectors for individual words and then average them to get the document embeddings. The resulting embeddings can be used for various tasks such as document similarity computation or classification.\n",
        "\n",
        "### Where Avg Word2Vec is Used:\n",
        "\n",
        "Document Similarity: It is used to measure the similarity between documents by comparing their average Word2Vec embeddings.\n",
        "\n",
        "Text Classification: In text classification tasks, Avg Word2Vec can be used to represent documents and classify them into predefined categories.\n",
        "\n",
        "Information Retrieval: It helps in retrieving relevant documents based on their semantic similarity to a query document or search query.\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "Semantic Representation: It captures the semantic meaning of the entire document by considering the meanings of individual words.\n",
        "\n",
        "Dimensionality Reduction: By averaging the word vectors, the dimensionality of the document representation is reduced compared to using individual word vectors, which can be beneficial for downstream tasks.\n",
        "\n",
        "Robustness: It is robust to the varying lengths of documents since it produces a fixed-length vector representation for each document.\n",
        "\n",
        "### Disadvantages:\n",
        "\n",
        "Loss of Information: Averaging the word vectors may lead to loss of important information, especially if the document contains a large number of words with diverse meanings.\n",
        "\n",
        "Equal Weighting: It treats all words in the document equally, regardless of their importance or frequency. This may not accurately represent the significance of each word in the document."
      ],
      "metadata": {
        "id": "UsbXhN0aa7Aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lmhwjxea54C"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained Word2Vec model\n",
        "word2vec_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bpl5hetcF9-",
        "outputId": "cbc1c2fd-412d-4f70-a90e-3c0495a2661c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"I love dogs and cats.\",\n",
        "    \"Dogs are loyal companions.\"\n",
        "]"
      ],
      "metadata": {
        "id": "IQh2TTu9cJI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the documents\n",
        "tokenized_documents = [doc.lower().split() for doc in documents]\n",
        "tokenized_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzTO-4_pcOQQ",
        "outputId": "d6bd7920-7715-4089-ab26-6f217eea030d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i', 'love', 'dogs', 'and', 'cats.'],\n",
              " ['dogs', 'are', 'loyal', 'companions.']]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average Word2Vec vectors for each document\n",
        "avg_word2vec_vectors = []\n",
        "for doc_tokens in tokenized_documents:\n",
        "    doc_vector = []\n",
        "    for token in doc_tokens:\n",
        "        if token in word2vec_model:\n",
        "            doc_vector.append(word2vec_model[token])\n",
        "    if doc_vector:\n",
        "        avg_doc_vector = sum(doc_vector) / len(doc_vector)\n",
        "        avg_word2vec_vectors.append(avg_doc_vector)\n",
        "    else:\n",
        "        avg_word2vec_vectors.append([])"
      ],
      "metadata": {
        "id": "7pM3VJuzcOcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the average Word2Vec vectors for each document\n",
        "for i, vector in enumerate(avg_word2vec_vectors):\n",
        "    print(f\"Average Word2Vec vector for Document {i+1}:\")\n",
        "    print(vector)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3BkGRBxcOgr",
        "outputId": "b7a69ea2-142c-438b-9fa2-aad0fc92c23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Word2Vec vector for Document 1:\n",
            "[-0.0476888  -0.06144206 -0.00374349  0.20670573 -0.11165365  0.0620931\n",
            "  0.09285482 -0.08919271 -0.07666016  0.08528646  0.05952962 -0.29231772\n",
            " -0.08768717 -0.08959961 -0.07556152  0.10270182 -0.00219727  0.14632161\n",
            " -0.05493164 -0.0008138  -0.02685547  0.0164388   0.19335938 -0.0851237\n",
            " -0.03255208  0.07991537 -0.19677734  0.07519531  0.05989583 -0.10026041\n",
            " -0.05875651  0.08924866 -0.02050781  0.01318359 -0.01251221  0.09480795\n",
            " -0.07910156  0.08625793 -0.02856445  0.2010905   0.09368896 -0.14208984\n",
            "  0.09554037  0.05053711 -0.00764974 -0.06298828  0.18131511 -0.05440267\n",
            " -0.04048665  0.01668294  0.00061035  0.09667969  0.10139974  0.1428833\n",
            "  0.04736328  0.10099284  0.03393555 -0.05110677  0.0290273  -0.02998861\n",
            "  0.11507162  0.07448324 -0.02620443  0.08585612 -0.00203451 -0.14029948\n",
            " -0.07210287  0.0258433  -0.14542644  0.06233724  0.20914714  0.01139323\n",
            " -0.10559082  0.07613119 -0.18562825 -0.12516277  0.0016276  -0.05969238\n",
            "  0.1241862   0.07804362 -0.19840495  0.02294922  0.1003418  -0.14290364\n",
            " -0.20806885  0.03792318 -0.18383789  0.22460938  0.05859375 -0.08813477\n",
            " -0.03468831  0.13513184 -0.08473714 -0.05861409  0.01489258 -0.07535807\n",
            "  0.03613281 -0.0777181   0.09407552 -0.11897787 -0.09733073  0.02056885\n",
            "  0.1274414   0.1451009   0.13435872 -0.00659053 -0.07511393 -0.0349528\n",
            "  0.07132975 -0.05761719 -0.16597493  0.04817708  0.09509277  0.03847249\n",
            "  0.2667643   0.03173828  0.03914388 -0.26595053 -0.0797933  -0.0875651\n",
            " -0.02929688  0.02258301 -0.16815186 -0.04720052  0.00358073 -0.14095052\n",
            " -0.17114258 -0.01761881  0.07161459 -0.03068034 -0.14892578 -0.09822591\n",
            " -0.14778645  0.05399577 -0.1529134  -0.00832367  0.04101562 -0.12243652\n",
            "  0.19694011  0.05889893  0.0304362  -0.00146484  0.02215067  0.04410807\n",
            " -0.07763672  0.04937744 -0.11873373  0.01188151 -0.11039225 -0.21289062\n",
            "  0.25016275  0.07486979 -0.0953776   0.00651042 -0.23242188  0.03190104\n",
            " -0.03173828 -0.06640625 -0.11328125 -0.05297852  0.0686849   0.05513509\n",
            " -0.00919596  0.20347087  0.01566569 -0.10709635  0.06835938  0.05761719\n",
            " -0.10286459  0.02555338 -0.10123698 -0.05444336  0.01149495  0.06013997\n",
            " -0.01582845  0.14607747  0.16105144 -0.05834961 -0.13671875 -0.11930338\n",
            " -0.09277344 -0.07743327  0.0046374   0.05325317 -0.00262451 -0.03971354\n",
            "  0.02636719  0.08789062  0.04752604  0.06738281  0.0123291   0.07259115\n",
            " -0.10970052  0.03873698  0.18880208  0.18652344 -0.02604167 -0.07161459\n",
            " -0.14011638 -0.13118489 -0.09627279 -0.03515625 -0.10685221 -0.1372884\n",
            " -0.03881836 -0.09676107 -0.09838867  0.08544922  0.06184896  0.0822347\n",
            " -0.15625     0.09375    -0.1347351  -0.08268229 -0.2076823  -0.04752604\n",
            "  0.22753906 -0.04785156 -0.21647136 -0.07969666 -0.08658854  0.06803385\n",
            "  0.06233724 -0.01057943 -0.07641602 -0.16113281  0.02801514 -0.01108805\n",
            "  0.00992839 -0.1944987  -0.03464762  0.09049479 -0.03428141  0.16577148\n",
            "  0.15966797  0.04610189  0.09667969  0.11121622  0.09505209  0.03003947\n",
            "  0.12125651 -0.03845215 -0.06152344 -0.0218099  -0.08105469  0.20133464\n",
            " -0.05755615  0.12434896 -0.01089728 -0.13330078  0.02132162  0.21907552\n",
            " -0.05110677  0.12475586  0.01302083 -0.04814148 -0.03753662 -0.09903971\n",
            " -0.1762085  -0.02416992 -0.03515625 -0.02209473 -0.00748698  0.05615234\n",
            " -0.04085286 -0.06298828 -0.05338542 -0.08662923 -0.08011881  0.0250651\n",
            " -0.03287761  0.06022136  0.02213542  0.2141927   0.16764323 -0.14835612\n",
            " -0.07779948  0.00846354 -0.0867513  -0.02034505  0.168396    0.01000977\n",
            "  0.1258138   0.1438802  -0.00154622 -0.03035482 -0.02008057  0.03352864\n",
            " -0.01090495  0.11971029 -0.04988607  0.15201823 -0.20930989 -0.07405599\n",
            "  0.05623372 -0.14949544 -0.02396647  0.06050618 -0.14257812  0.06032308]\n",
            "\n",
            "Average Word2Vec vector for Document 2:\n",
            "[ 0.02539062  0.03849284  0.0012207   0.14680989 -0.26204428  0.0847168\n",
            " -0.07832845 -0.06530762  0.00691732  0.03804525  0.03352864 -0.13208008\n",
            "  0.01318359  0.14835612 -0.04496257  0.06095378 -0.02750651  0.2233073\n",
            " -0.16235352  0.09757487  0.09700521 -0.1303711   0.1404012  -0.07137045\n",
            "  0.06884766  0.03287761 -0.1764323   0.12483724  0.07486979 -0.24153645\n",
            " -0.00537109 -0.06667582 -0.034729    0.03841146  0.14796956  0.04500325\n",
            " -0.03686523  0.03258769 -0.04020182  0.08654785  0.19677734 -0.06445312\n",
            " -0.02067057  0.01729329 -0.03271484 -0.17154948 -0.0005188   0.04545085\n",
            " -0.09749349 -0.07405599  0.00880941  0.09960938  0.04984538  0.17252605\n",
            "  0.08825684  0.0550944   0.16699219 -0.0279541  -0.00785319  0.00732422\n",
            " -0.02522787  0.21679688 -0.02929688  0.04817708 -0.01015218 -0.07278442\n",
            " -0.05419922  0.02449544 -0.15816243  0.09436035  0.15865071  0.04923503\n",
            " -0.06787109 -0.06066895 -0.10538737 -0.09960938  0.04797363 -0.03129069\n",
            "  0.10990397  0.05973307 -0.06193034 -0.015625    0.18863933  0.01432292\n",
            " -0.18310547 -0.1866862  -0.27539062  0.12426758  0.07210287 -0.11720148\n",
            " -0.13291423  0.01737467 -0.03104655 -0.07983398  0.13085938 -0.18408203\n",
            " -0.00374349 -0.03389486  0.01546224 -0.10457357 -0.02669271 -0.01627604\n",
            "  0.07682291  0.18033855  0.11762492 -0.21451823  0.08064779 -0.02319336\n",
            "  0.17952473  0.01713053 -0.00113932  0.03673299  0.03049723  0.08841959\n",
            "  0.0773112   0.02852376  0.04394531 -0.24967448  0.04280599 -0.11393229\n",
            " -0.12613933  0.13411458 -0.1554362  -0.08219401  0.0094401   0.01764552\n",
            " -0.06510416 -0.07063802  0.08186849 -0.07027181  0.01529948 -0.11010742\n",
            "  0.03971354  0.04003906 -0.10424805 -0.0390625   0.20019531 -0.15266927\n",
            "  0.1373698   0.11775716  0.00333659  0.1126709   0.04737854 -0.02372233\n",
            "  0.01137288  0.10270182 -0.00667318 -0.13427734 -0.03100586 -0.13252766\n",
            "  0.03092448  0.19938152 -0.03027344 -0.06152344 -0.1101888   0.04614258\n",
            " -0.03027344 -0.12434896 -0.04082743  0.07820638 -0.09554037  0.08532715\n",
            "  0.02901204  0.05487061  0.06103516 -0.07242838 -0.08642578 -0.00541178\n",
            "  0.03841146 -0.03767904 -0.21744792 -0.12263998  0.0686849  -0.02929688\n",
            "  0.0012207  -0.06013997 -0.11409505  0.00569661 -0.12095133  0.02001953\n",
            " -0.08577474 -0.09061686  0.02832031  0.01009115 -0.0754598  -0.09879557\n",
            "  0.14453125  0.20833333  0.12223307  0.18587239 -0.07259115  0.04390462\n",
            "  0.0275472  -0.11311849 -0.06030273  0.16276042  0.02986654 -0.05045573\n",
            " -0.2101237   0.03796387  0.03584798  0.09407552 -0.01220703 -0.0887858\n",
            " -0.0970459  -0.08544922 -0.06111654 -0.03796387  0.06530762  0.00073242\n",
            " -0.15445964  0.02083333  0.06136068  0.0641276  -0.10087077  0.09564209\n",
            "  0.17285156 -0.14501953 -0.2607422   0.02718099  0.02913412 -0.08953857\n",
            " -0.08646647 -0.08463541  0.10286459 -0.0842692   0.05403646  0.00152588\n",
            "  0.09082031 -0.10384115 -0.01885986 -0.02339681 -0.04052734  0.2039388\n",
            "  0.06689453  0.06152344  0.06933594  0.02608236 -0.04231771 -0.0226237\n",
            "  0.02978516 -0.05501302 -0.03597005 -0.03800456  0.06144206  0.19759114\n",
            "  0.03641764  0.13427734 -0.08479818  0.02213542 -0.14078777  0.1476237\n",
            " -0.0657959   0.01391602 -0.08015951  0.03572591 -0.0766805   0.07763672\n",
            " -0.19047038 -0.04044596  0.03613281 -0.05187988 -0.056722    0.28434244\n",
            " -0.02832031  0.07275391 -0.21891277 -0.04707845 -0.08487955  0.01953125\n",
            "  0.12304688  0.1728719  -0.1616211   0.19270833  0.09151205 -0.046875\n",
            "  0.00337728 -0.06754557  0.07324219 -0.04899089  0.11246745 -0.02612305\n",
            "  0.04612223 -0.00528971  0.02604167  0.00649007 -0.18186443  0.10310873\n",
            "  0.18977864  0.05371094 -0.06852213  0.06673177 -0.19132487  0.02124023\n",
            "  0.06154378  0.02026367 -0.00891113  0.23042805  0.05110677  0.10286459]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Similarity score close to 0 indicates little to no similarity between the documents,\n",
        "    while a score close to 1 indicates high similarity. Conversely, a score close to -1 would indicate high dissimilarity."
      ],
      "metadata": {
        "id": "0jT3jpydreIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculate cosine similarity between the vectors of the two documents\n",
        "similarity_score = cosine_similarity([avg_word2vec_vectors[0]], [avg_word2vec_vectors[1]])\n",
        "\n",
        "# Print the similarity score\n",
        "print(\"Similarity Score:\", similarity_score[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8SsoNxmcOkU",
        "outputId": "790f50fa-83ce-4cc1-e19f-8449ba4933ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Score: 0.56154925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nyfrEitWr9Pj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}